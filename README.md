# LightWeight-FineTuning-to-Foundation-Model
This project explores Parameter-Efficient Fine-Tuning (PEFT) using LoRA (Low-Rank Adaptation) on a lightweight transformer model for binary sequence classification. Instead of fine-tuning all model parameters, LoRA introduces additional trainable weights into a few layers of the model.
